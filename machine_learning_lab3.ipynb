{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "machine_learning_lab3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNJiHpKHO/JPuJWVW0hGz/S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diogocezar/phd-machine-learning-lab3/blob/master/machine_learning_lab3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkY044wF9IJ9",
        "colab_type": "text"
      },
      "source": [
        "# LABORATÓRIO 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fL8MOm6cJIUg",
        "colab_type": "text"
      },
      "source": [
        "## INTRODUÇÃO\n",
        "\n",
        "A ideia deste notebook é criar um experimento para que a atividade de laboratório 3 da disciplina de aprenziagem de máquina.\n",
        "\n",
        "### ENUNCIADO\n",
        "\n",
        "Para esse laboratório considere a base de dados de meses do ano (12 classes) apresentado nas práticas de Deep Learning.\n",
        "\n",
        "1. Implemente funções para aumentar o número de amostras do conjunto de TREINAMENTO (Data Augmentation);\n",
        "\n",
        "2. Implemente duas redes neurais convolucionais: (a) LeNet 5 [Yann LeCun (1998)]; (b) CNN de sua escolha;\n",
        "\n",
        "3. Escreva um breve relatório que:\n",
        "  - Descreva as CNNs utilizadas e as funções de Data Augmentation;\n",
        "  - Compare o desempenho dessas redes variando os diferentes parâmetros apresentados em aula. Realize treinamentos com e sem Data Augmentation. Analise os resultados obtidos nos diferentes experimentos apresentando suas conclusões (apresente gráficos e matriz de confusão);\n",
        "  - Repita os experimentos e análises utilizando 2 redes pré-treinadas na ImageNet (Transfer Learning/Fine-Tuning). Utilize essas redes para gerar vetores de características, realizando a classificação em outro classificador (Ex: SVM).\n",
        "\n",
        "\n",
        "### LINKS ÚTEIS\n",
        "\n",
        "- https://github.com/TaavishThaman/LeNet-5-with-Keras/blob/master/lenet_5.py\n",
        "- https://engmrk.com/lenet-5-a-classic-cnn-architecture/\n",
        "- https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/\n",
        "- https://engmrk.com/alexnet-implementation-using-keras/\n",
        "- https://cs231n.github.io/convolutional-networks/\n",
        "- https://github.com/eweill/keras-deepcv/blob/master/models/classification/alexnet.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yvWhVbBRNPA",
        "colab_type": "text"
      },
      "source": [
        "# PREPARAÇÕES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEO7aBaNbC9o",
        "colab_type": "text"
      },
      "source": [
        "## HABILITANDO A GPU\n",
        "\n",
        "Neste ponto se importa o tensorflow e executa-se uma verificação para analisar se a máquina que executará os scripts utiliza o processamento em GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmH7u-DNRbV7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "print('HABILITANDO GPU - DONE')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3A8EHmvmH5k",
        "colab_type": "text"
      },
      "source": [
        "Caso a saída seja: `Found...` então, pode-se executar a próxia linha, que habilita a execução."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbXZJ0pnRmwu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaxQD5tjJpXz",
        "colab_type": "text"
      },
      "source": [
        "## IMPORTANDO OS DADOS DO GITHUB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFCz7vSKmTal",
        "colab_type": "text"
      },
      "source": [
        "Neste ponto, utiliza-se o repositório do GitHub listado abaixo para obter o .zip com as informações a serem utilizadas no experimento.\n",
        "\n",
        "Os comandos na sequência removem as pastas desnecessárias deixando apenas os arquivos importantes para a execução dos próximos passos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-ET-26UQgFm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone --single-branch --branch data https://github.com/diogocezar/phd-machine-learning-lab3 ./data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4ZkxWuN_EXy",
        "colab_type": "text"
      },
      "source": [
        "## REALIZANDO OS IMPORTS\n",
        "\n",
        "Aqui reliza-se as importações dos pacotes que serão utilizados nos experimentos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjFWKddV_K6u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.datasets import load_svmlight_file\n",
        "from sklearn.metrics import f1_score as sklearn_f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import svm\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Activation, Conv2D, MaxPooling2D, AveragePooling2D, ZeroPadding2D, BatchNormalization\n",
        "from keras.models import Model\n",
        "from keras.regularizers import l2\n",
        "\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "\n",
        "import os\n",
        "\n",
        "print('REALIZANDO OS IMPORTS - DONE')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZK6raP3_k1b",
        "colab_type": "text"
      },
      "source": [
        "## DEFININDO AS ENTRADAS\n",
        "\n",
        "Então, é necessário definir os arquivos de entrada obtidos através do GitHub."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykl0sbD8_r54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_FILE = \"./data/train.txt\"\n",
        "TEST_FILE = \"./data/test.txt\"\n",
        "\n",
        "print('DEFININDO AS ENTRADAS - DONE')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04wdIy9n_661",
        "colab_type": "text"
      },
      "source": [
        "## DEFININDO AS FUNÇÕES AUXILIARES\n",
        "\n",
        "Neste ponto são definidas as funções auxiliares para a execução dos experimentos.**negrito**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vui_BFAAEju",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resize_data(data, size, convert):\n",
        "\tif convert:\n",
        "\t\tdata_upscaled = np.zeros((data.shape[0], size[0], size[1], 3))\n",
        "\telse:\n",
        "\t\tdata_upscaled = np.zeros((data.shape[0], size[0], size[1]))\n",
        "\tfor i, img in enumerate(data):\n",
        "\t\tlarge_img = cv2.resize(img, dsize=(size[1], size[0]), interpolation=cv2.INTER_CUBIC)\n",
        "\t\tdata_upscaled[i] = large_img\n",
        "\treturn data_upscaled\n",
        "\n",
        "def load_images(image_paths, convert=False):\n",
        "\tx = []\n",
        "\ty = []\n",
        "\tfor image_path in image_paths:\n",
        "\t\tpath, label = image_path.split(' ')\n",
        "\t\tpath= './data/data/' + path\n",
        "\t\tif convert:\n",
        "\t\t\timage_pil = Image.open(path).convert('RGB') \n",
        "\t\telse:\n",
        "\t\t\timage_pil = Image.open(path).convert('L')\n",
        "\t\timg = np.array(image_pil, dtype=np.uint8)\n",
        "\t\tx.append(img)\n",
        "\t\ty.append([int(label)])\n",
        "\tx = np.array(x)\n",
        "\ty = np.array(y)\n",
        "\tif np.min(y) != 0: \n",
        "\t\ty = y-1\n",
        "\treturn x, y\n",
        "\n",
        "def load_dataset(train_file, test_file, resize, convert=False, size=(224,224)):\n",
        "\tarq = open(train_file, 'r')\n",
        "\ttexto = arq.read()\n",
        "\ttrain_paths = texto.split('\\n')\n",
        "\tprint ('Size:', size)\n",
        "\ttrain_paths.remove('')\n",
        "\ttrain_paths.sort()\n",
        "\tprint (\"Loading training set...\")\n",
        "\tx_train, y_train = load_images(train_paths, convert)\n",
        "\tarq = open(test_file, 'r')\n",
        "\ttexto = arq.read()\n",
        "\ttest_paths = texto.split('\\n')\n",
        "\ttest_paths.remove('')\n",
        "\ttest_paths.sort()\n",
        "\tprint (\"Loading testing set...\")\n",
        "\tx_test, y_test = load_images(test_paths, convert)\n",
        "\tif resize:\n",
        "\t\tprint (\"Resizing images...\")\n",
        "\t\tx_train = resize_data(x_train, size, convert)\n",
        "\t\tx_test = resize_data(x_test, size, convert)\n",
        "\tif not convert:\n",
        "\t\tx_train = x_train.reshape(x_train.shape[0], size[0], size[1], 1)\n",
        "\t\tx_test = x_test.reshape(x_test.shape[0], size[0], size[1], 1)\n",
        "\tprint (np.shape(x_train))\n",
        "\treturn (x_train, y_train), (x_test, y_test)\n",
        " \n",
        "def generate_labels(x_test, y_test):\n",
        "  labels = []\n",
        "  for i in range(len(x_test)):\n",
        "    labels.append(y_test[i][0])\n",
        "  return labels\n",
        "\n",
        "def normalize_images(x):\n",
        "  x = x.astype('float32')\n",
        "  x /= 255\n",
        "  return x\n",
        "\n",
        "def convert_vector(x, num_classes):\n",
        "  return keras.utils.to_categorical(x, num_classes)\n",
        "\n",
        "def fit_model(model, x_train, y_train, x_test, y_test, epochs, batch_size=128, verbose=1):\n",
        "  return model.fit(x=x_train, y=y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, y_test), verbose=verbose)\n",
        "\n",
        "def get_confusion_matrix(model, x_test, labels):\n",
        "  pred = []\n",
        "  y_pred = model.predict_classes(x_test)\n",
        "  for i in range(len(x_test)):\n",
        "    pred.append(y_pred[i])\n",
        "  return confusion_matrix(labels, pred)\n",
        "\n",
        "def plot_graphs(history):\n",
        "  acc = history.history['accuracy']\n",
        "  val_acc = history.history['val_accuracy']\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "  epochs = range(len(acc))\n",
        "  plt.plot(epochs, acc, 'b', label='Acurácia do treinamento')\n",
        "  plt.plot(epochs, val_acc, 'r', label='Acurácia da validação')\n",
        "  plt.title('Acurácia do treinamento e validação')\n",
        "  plt.legend()\n",
        "  plt.figure()\n",
        "  plt.plot(epochs, loss, 'b', label='Perda do treinamento')\n",
        "  plt.plot(epochs, val_loss, 'r', label='Perda da validação')\n",
        "  plt.title('Perda do treinamento e validação')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "def extract_features(input_file, output_file, img_rows, img_cols, dir_dataset):\n",
        "\tfile_input = open (input_file, 'r')\n",
        "\tinput = file_input.readlines()\n",
        "\tfile_input.close()\n",
        "\toutput = open(output_file, 'w')\n",
        "\tmodel = InceptionV3(weights='imagenet', include_top=False)\n",
        "\tfor i in input:\n",
        "\t\tsample_name, sample_class = i.split()\n",
        "\t\timg_path = dir_dataset + sample_name\n",
        "\t\tprint(img_path)\n",
        "\t\timg = image.load_img(img_path, target_size=(img_rows,img_cols))\n",
        "\t\timg_data = image.img_to_array(img)\n",
        "\t\timg_data = np.expand_dims(img_data, axis=0)\n",
        "\t\timg_data = preprocess_input(img_data)\n",
        "\t\tinception_features = model.predict(img_data)\n",
        "\t\tfeatures_np = np.array(inception_features)\n",
        "\t\tfeatures_np = features_np.flatten()\n",
        "\t\toutput.write(sample_class+' ')\n",
        "\t\tfor j in range (features_np.size):\n",
        "\t\t\toutput.write(str(j+1)+':'+str(features_np[j])+' ')\n",
        "\t\toutput.write('\\n')\n",
        "\tprint(features_np.size)\n",
        "\toutput.close()\n",
        " \n",
        "def round_float(value):\n",
        "\treturn float(\"{:.3f}\".format(value))\n",
        "\n",
        "print('DEFININDO AS FUNÇÕES AUXILIARES - DONE')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EApxFTnPb9OK",
        "colab_type": "text"
      },
      "source": [
        "# DEFININDO OS MODELOS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WCiMcSJPs_i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lenet5(img_rows, img_cols, num_classes):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(6, kernel_size=(5, 5), strides=(1, 1), activation='tanh', input_shape=(img_rows,img_cols,3), padding=\"same\"))\n",
        "  model.add(AveragePooling2D(pool_size=(2, 2), strides=(1, 1), padding='valid'))\n",
        "  model.add(Conv2D(16, kernel_size=(5, 5), strides=(1, 1), activation='tanh', padding='valid'))\n",
        "  model.add(AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n",
        "  model.add(Conv2D(120, kernel_size=(5, 5), strides=(1, 1), activation='tanh', padding='valid'))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(84, activation='tanh'))\n",
        "  model.add(Dense(num_classes, activation='softmax'))\n",
        "  return model\n",
        "\n",
        "def default(img_rows, img_cols, num_classes):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(img_rows,img_cols,3)))\n",
        "  model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(num_classes, activation='softmax'))\n",
        "  return model\n",
        "\n",
        "print('DEFININDO OS MODELOS - DONE')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4_H_F64J4aj",
        "colab_type": "text"
      },
      "source": [
        "# LENET 5 - SEM DATA AUGMENTATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1Z2wX8EJKuW",
        "colab_type": "text"
      },
      "source": [
        "## TREINANDO O MODELO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inwNxcv1Z6jC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Definitions\n",
        "\n",
        "NUM_CLASSES = 12\n",
        "IMG_ROWS = 32\n",
        "IMG_COLS = 32\n",
        "EPOCHS = 64\n",
        "\n",
        "## Loading Inital Data\n",
        "(x_train, y_train), (x_test, y_test) = load_dataset(TRAIN_FILE, TEST_FILE, resize=True, convert=True, size=(IMG_ROWS, IMG_COLS))\n",
        "\n",
        "## Normalize images\n",
        "x_train = normalize_images(x_train)\n",
        "x_test = normalize_images(x_test)\n",
        "\n",
        "## Generating Labels for Confusion Matrix\n",
        "labels = generate_labels(x_test, y_test)\n",
        "\n",
        "## Convert class vectros to binary class matrices\n",
        "y_train = convert_vector(y_train, NUM_CLASSES)\n",
        "y_test = convert_vector(y_test, NUM_CLASSES)\n",
        "\n",
        "## Get LeNet 5 Model\n",
        "model = lenet5(IMG_ROWS, IMG_COLS, NUM_CLASSES)\n",
        "\n",
        "## Printing Summary\n",
        "model.summary()\n",
        "\n",
        "## Compiling Model\n",
        "model.compile(loss=keras.losses.categorical_crossentropy, optimizer='SGD', metrics=[\"accuracy\"])\n",
        "\n",
        "## Trainning model\n",
        "history = fit_model(model, x_train, y_train, x_test, y_test, EPOCHS)\n",
        "\n",
        "print('TREINANDO O MODELO - DONE')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNFO6YkRFv9p",
        "colab_type": "text"
      },
      "source": [
        "## OBTENDO OS RESULTADOS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CGUaDHJal6X",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivvt9yfBbURS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Getting Score\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "## Loss\n",
        "print('Loss:', score[0])\n",
        "\n",
        "## Accuracy\n",
        "print('Accuracy:', score[1])\n",
        "\n",
        "## Confusion Matrix\n",
        "cm = get_confusion_matrix(model, x_test, labels)\n",
        "print(f'Confusion Matrix: \\n {cm}')\n",
        "\n",
        "## Graphs\n",
        "plot_graphs(history)\n",
        "\n",
        "print('OBTENDO OS RESULTADOS - DONE')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pY7x4C3MKDvK",
        "colab_type": "text"
      },
      "source": [
        "# OUTRO MODELO CNN - SEM DATA AUGMENTATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzMN-3OwRB9F",
        "colab_type": "text"
      },
      "source": [
        "## TREINANDO O MODELO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAjksh2jRDfU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Definitions\n",
        "\n",
        "NUM_CLASSES = 12\n",
        "IMG_ROWS = 64\n",
        "IMG_COLS = 64\n",
        "EPOCHS = 64\n",
        "\n",
        "## Loading Inital Data\n",
        "(x_train, y_train), (x_test, y_test) = load_dataset(TRAIN_FILE, TEST_FILE, resize=True, convert=True, size=(IMG_ROWS, IMG_COLS))\n",
        "\n",
        "## Normalize images\n",
        "x_train = normalize_images(x_train)\n",
        "x_test = normalize_images(x_test)\n",
        "\n",
        "## Generating Labels for Confusion Matrix\n",
        "labels = generate_labels(x_test, y_test)\n",
        "\n",
        "## Convert class vectros to binary class matrices\n",
        "y_train = convert_vector(y_train, NUM_CLASSES)\n",
        "y_test = convert_vector(y_test, NUM_CLASSES)\n",
        "\n",
        "## Get LeNet 5 Model\n",
        "model = default(IMG_ROWS, IMG_COLS, NUM_CLASSES)\n",
        "\n",
        "## Printing Summary\n",
        "model.summary()\n",
        "\n",
        "## Compiling Model\n",
        "model.compile(loss=keras.losses.categorical_crossentropy, optimizer='SGD', metrics=[\"accuracy\"])\n",
        "\n",
        "## Trainning model\n",
        "history = fit_model(model, x_train, y_train, x_test, y_test, EPOCHS)\n",
        "\n",
        "print('TREINANDO O MODELO - DONE')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVa2kuHGcmqL",
        "colab_type": "text"
      },
      "source": [
        "## OBTENDO OS RESULTADOS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULV8MLAtI6Vq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Getting Score\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "# Loss\n",
        "print('Loss:', score[0])\n",
        "\n",
        "# Accuracy\n",
        "print('Accuracy:', score[1])\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = get_confusion_matrix(model, x_test, labels)\n",
        "print(f'Confusion Matrix: \\n {cm}')\n",
        "\n",
        "# Graphs\n",
        "plot_graphs(history)\n",
        "\n",
        "print('OBTENDO OS RESULTADOS - DONE')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaq2HBGXu2NY",
        "colab_type": "text"
      },
      "source": [
        "# IMPLEMENTANDO DATA AUGMENTATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SKULvmm_3oY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir ./data/data-aug\n",
        "!mkdir ./data/data-aug/all\n",
        "!mkdir ./data/data-aug/brightness\n",
        "!mkdir ./data/data-aug/flip\n",
        "!mkdir ./data/data-aug/rotation\n",
        "!mkdir ./data/data-aug/shift\n",
        "!mkdir ./data/data-aug/zoom\n",
        "!touch ./data/train-aug.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsOk1k3c_ynt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy import expand_dims\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from matplotlib import pyplot\n",
        "import glob\n",
        "import os\n",
        "from tempfile import mkstemp\n",
        "from shutil import move, copymode\n",
        "from os import fdopen, remove\n",
        "\n",
        "#https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/\n",
        "\n",
        "drive_path = './'\n",
        "aug_path = drive_path + \"data-aug\"\n",
        "train_file_aug = drive_path + 'train-aug.txt'\n",
        "\n",
        "\n",
        "def replace(file_path, pattern):\n",
        "    #Create temp file\n",
        "    fh, abs_path = mkstemp()\n",
        "    with fdopen(fh,'w') as new_file:\n",
        "        with open(file_path) as old_file:\n",
        "            for line in old_file:\n",
        "                if pattern in line:\n",
        "                    continue\n",
        "                new_file.write(line)\n",
        "    #Copy the file permissions from the old file to the new file\n",
        "    copymode(file_path, abs_path)\n",
        "    #Remove original file\n",
        "    remove(file_path)\n",
        "    #Move new file\n",
        "    move(abs_path, file_path)\n",
        "\n",
        "def save_to_aug(label, subdir):\n",
        "    list_of_files = glob.glob(aug_path + \"/\" + subdir + \"/*\")  # * means all if need specific format then *.csv\n",
        "    if len(list_of_files) > 0:\n",
        "        latest_file = max(list_of_files, key=os.path.getctime)\n",
        "        replace(train_file_aug, latest_file.replace(aug_path + \"/\", \"\"))\n",
        "        arq_aug = open(train_file_aug, \"a+\")\n",
        "        arq_aug.write(latest_file.replace(aug_path + \"/\", \"\") + \" \" + label + \"\\n\")\n",
        "        arq_aug.close()\n",
        "\n",
        "\n",
        "def flip_rotation_brightness_zoom(path, zoom=[0.5, 1.0], brightness=[0.2, 1.0], rotation=90, flip_horizontal=False,\n",
        "                                  flip_vertical=False, subdir=\"all\"):\n",
        "    path, label = path.split(' ')\n",
        "    path = drive_path + 'data/' + path\n",
        "    img = load_img(path)\n",
        "    # convert to numpy array\n",
        "    data = img_to_array(img)\n",
        "    # expand dimension to one sample\n",
        "    samples = expand_dims(data, 0)\n",
        "    # create image data augmentation generator\n",
        "    datagen = ImageDataGenerator(zoom_range=zoom, brightness_range=brightness, rotation_range=rotation,\n",
        "                                 horizontal_flip=flip_horizontal,vertical_flip=flip_vertical)\n",
        "    # prepare iterator\n",
        "    it = datagen.flow(samples, save_to_dir=aug_path + \"/\" + subdir + \"/\", batch_size=1)\n",
        "    save_to_aug(label, subdir)\n",
        "    # generate samples and plot\n",
        "    for i in range(1):\n",
        "        # define subplot\n",
        "        pyplot.subplot(330 + 1 + i)\n",
        "        # generate batch of images\n",
        "        batch = it.next()\n",
        "        # convert to unsigned integers for viewing\n",
        "        image = batch[0].astype('uint8')\n",
        "        # plot raw pixel data\n",
        "        pyplot.imshow(image)\n",
        "    # show the figure\n",
        "    # pyplot.show()\n",
        "\n",
        "\n",
        "def random_zoom(path, zoom=[0.5, 1.0], subdir=\"zoom\"):\n",
        "    path, label = path.split(' ')\n",
        "    path = drive_path + 'data/' + path\n",
        "    img = load_img(path)\n",
        "    # convert to numpy array\n",
        "    data = img_to_array(img)\n",
        "    # expand dimension to one sample\n",
        "    samples = expand_dims(data, 0)\n",
        "    # create image data augmentation generator\n",
        "    datagen = ImageDataGenerator(zoom_range=zoom)\n",
        "    # prepare iterator\n",
        "    it = datagen.flow(samples, save_to_dir=aug_path + \"/\" + subdir + \"/\", batch_size=1)\n",
        "    save_to_aug(label, subdir)\n",
        "    # generate samples and plot\n",
        "    for i in range(1):\n",
        "        # define subplot\n",
        "        pyplot.subplot(330 + 1 + i)\n",
        "        # generate batch of images\n",
        "        batch = it.next()\n",
        "        # convert to unsigned integers for viewing\n",
        "        image = batch[0].astype('uint8')\n",
        "        # plot raw pixel data\n",
        "        pyplot.imshow(image)\n",
        "    # show the figure\n",
        "    #pyplot.show()\n",
        "\n",
        "\n",
        "def random_brightness(path, brightness=[0.2, 1.0], subdir=\"brightness\"):\n",
        "    path, label = path.split(' ')\n",
        "    path = drive_path + 'data/' + path\n",
        "    # load the image\n",
        "    img = load_img(path)\n",
        "    # convert to numpy array\n",
        "    data = img_to_array(img)\n",
        "    # expand dimension to one sample\n",
        "    samples = expand_dims(data, 0)\n",
        "    # create image data augmentation generator\n",
        "    datagen = ImageDataGenerator(brightness_range=brightness)\n",
        "    # prepare iterator\n",
        "    it = datagen.flow(samples, save_to_dir=aug_path + \"/\" + subdir + \"/\", batch_size=1)\n",
        "    save_to_aug(label, subdir)\n",
        "    # generate samples and plot\n",
        "    for i in range(1):\n",
        "        # define subplot\n",
        "        pyplot.subplot(330 + 1 + i)\n",
        "        # generate batch of images\n",
        "        batch = it.next()\n",
        "        # convert to unsigned integers for viewing\n",
        "        image = batch[0].astype('uint8')\n",
        "        # plot raw pixel data\n",
        "        pyplot.imshow(image)\n",
        "    # show the figure\n",
        "    #pyplot.show()\n",
        "\n",
        "\n",
        "def random_rotation(path, rotation=90, subdir=\"rotation\"):\n",
        "    path, label = path.split(' ')\n",
        "    path = drive_path + 'data/' + path\n",
        "    img = load_img(path)\n",
        "    # convert to numpy array\n",
        "    data = img_to_array(img)\n",
        "    # expand dimension to one sample\n",
        "    samples = expand_dims(data, 0)\n",
        "    # create image data augmentation generator\n",
        "    datagen = ImageDataGenerator(rotation_range=rotation)\n",
        "    # prepare iterator\n",
        "    it = datagen.flow(samples, save_to_dir=aug_path + \"/\" + subdir + \"/\", batch_size=1)\n",
        "    save_to_aug(label, subdir)\n",
        "    # generate samples and plot\n",
        "    for i in range(1):\n",
        "        # define subplot\n",
        "        pyplot.subplot(330 + 1 + i)\n",
        "        # generate batch of images\n",
        "        batch = it.next()\n",
        "        # convert to unsigned integers for viewing\n",
        "        image = batch[0].astype('uint8')\n",
        "        # plot raw pixel data\n",
        "        pyplot.imshow(image)\n",
        "    # show the figure\n",
        "    #pyplot.show()\n",
        "\n",
        "\n",
        "def horizontal_vertical_flip(path, flip_horizontal=False, flip_vertical=False, subdir=\"flip\"):\n",
        "    path, label = path.split(' ')\n",
        "    path = drive_path + 'data/' + path\n",
        "    # load the image\n",
        "    img = load_img(path)\n",
        "    # convert to numpy array\n",
        "    data = img_to_array(img)\n",
        "    # expand dimension to one sample\n",
        "    samples = expand_dims(data, 0)\n",
        "    # create image data augmentation generator\n",
        "    datagen = ImageDataGenerator(horizontal_flip=flip_horizontal,vertical_flip=flip_vertical)\n",
        "    # prepare iterator\n",
        "    it = datagen.flow(samples, save_to_dir=aug_path + \"/\" + subdir + \"/\", batch_size=1)\n",
        "    save_to_aug(label, subdir)\n",
        "    # generate samples and plot\n",
        "    for i in range(1):\n",
        "        # define subplot\n",
        "        pyplot.subplot(330 + 1 + i)\n",
        "        # generate batch of images\n",
        "        batch = it.next()\n",
        "        # convert to unsigned integers for viewing\n",
        "        image = batch[0].astype('uint8')\n",
        "        # plot raw pixel data\n",
        "        pyplot.imshow(image)\n",
        "    # show the figure\n",
        "    #pyplot.show()\n",
        "\n",
        "\n",
        "def horizontal_vertical_shift(path, size=0.5, bool_width=True, subdir=\"shift\"):\n",
        "    path, label = path.split(' ')\n",
        "    path = drive_path + 'data/' + path\n",
        "    # load the image\n",
        "    img = load_img(path)\n",
        "    # convert to numpy array\n",
        "    data = img_to_array(img)\n",
        "    # expand dimension to one sample\n",
        "    samples = expand_dims(data, 0)\n",
        "    # create image data augmentation generator\n",
        "    if bool_width:\n",
        "        datagen = ImageDataGenerator(width_shift_range=size)\n",
        "    else:\n",
        "        datagen = ImageDataGenerator(height_shift_range=size)\n",
        "    # prepare iterator\n",
        "    it = datagen.flow(samples, save_to_dir=aug_path + \"/\" + subdir + \"/\", batch_size=1)\n",
        "    save_to_aug(label, subdir)\n",
        "    # generate samples and plot\n",
        "    for i in range(1):\n",
        "        # define subplot\n",
        "        pyplot.subplot(330 + 1 + i)\n",
        "        # generate batch of images\n",
        "        batch = it.next()\n",
        "        # convert to unsigned integers for viewing\n",
        "        image = batch[0].astype('uint8')\n",
        "        # plot raw pixel data\n",
        "        pyplot.imshow(image)\n",
        "    # show the figure\n",
        "    #pyplot.show()\n",
        "\n",
        "\n",
        "# Train and Test files\n",
        "train_file = drive_path + 'train.txt'\n",
        "\n",
        "arq = open(train_file, 'r')\n",
        "texto = arq.read()\n",
        "train_paths = texto.split('\\n')\n",
        "\n",
        "train_paths.remove('')  # Remove empty lines\n",
        "train_paths.sort()\n",
        "\n",
        "for image_path in train_paths:\n",
        "    horizontal_vertical_shift(image_path, bool_width=True)\n",
        "    horizontal_vertical_shift(image_path, bool_width=False)\n",
        "    random_rotation(image_path, rotation=30)\n",
        "    random_rotation(image_path, rotation=45)\n",
        "    random_brightness(image_path)\n",
        "    random_brightness(image_path, brightness=[0.1, 0.2])\n",
        "    random_zoom(image_path)\n",
        "    random_zoom(image_path, zoom=[0.1, 0.5])\n",
        "    flip_rotation_brightness_zoom(image_path, rotation=30)\n",
        "    flip_rotation_brightness_zoom(image_path, zoom=[0.1, 0.5], brightness=[0.1, 0.5])\n",
        "    flip_rotation_brightness_zoom(image_path, zoom=[0.1, 0.5], brightness=[0.1, 0.5])\n",
        "    flip_rotation_brightness_zoom(image_path, zoom=[0.1, 0.5], brightness=[0.1, 0.5], rotation=30)\n",
        "    flip_rotation_brightness_zoom(image_path, zoom=[0.1, 0.5], brightness=[0.1, 0.5], rotation=30)\n",
        "    flip_rotation_brightness_zoom(image_path, zoom=[0.1, 0.5], brightness=[0.1, 0.5], rotation=30)\n",
        "    flip_rotation_brightness_zoom(image_path, zoom=[0.1, 0.8], brightness=[0.1, 0.8], rotation=45)\n",
        "    flip_rotation_brightness_zoom(image_path, zoom=[0.1, 0.8], brightness=[0.1, 0.8], rotation=45)\n",
        "    flip_rotation_brightness_zoom(image_path, zoom=[0.1, 0.2], brightness=[0.1, 0.2], rotation=30)\n",
        "    flip_rotation_brightness_zoom(image_path, zoom=[0.1, 0.2], brightness=[0.1, 0.2], rotation=45)\n",
        "    flip_rotation_brightness_zoom(image_path, zoom=[0.9, 1], brightness=[0.9, 1], rotation=30)\n",
        "    flip_rotation_brightness_zoom(image_path, zoom=[0.9, 1], brightness=[0.9, 1], rotation=45)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7t_p3tMDvB97",
        "colab_type": "text"
      },
      "source": [
        "# LENET 5 - COM DATA AUGMENTATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sf6QusIvG40",
        "colab_type": "text"
      },
      "source": [
        "# OUTRO MODELO CNN - COM DATA AUGMENTATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhPPo5QReMne",
        "colab_type": "text"
      },
      "source": [
        "# EXTRAÇÃO DE CARACTERÍSTICAS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkzblMPziBLg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir ./data/svm\n",
        "!touch ./data/svm/test.svm\n",
        "!touch ./data/svm/train.svm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPKqNYryeRpe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_FILE_TEST = \"./data/test.txt\"\n",
        "OUTPUT_FILE_TEST = \"./data/svm/test.svm\"\n",
        "INPUT_FILE_TRAIN = \"./data/train.txt\"\n",
        "OUTPUT_FILE_TRAIN = \"./data/svm/train.svm\"\n",
        "IMG_ROWS = 100\n",
        "IMG_COLS = 100\n",
        "DIR_DATASET = \"./data/data/\"\n",
        "\n",
        "# Train\n",
        "extract_features(INPUT_FILE_TRAIN, OUTPUT_FILE_TRAIN, IMG_ROWS, IMG_COLS, DIR_DATASET)\n",
        "\n",
        "# Test\n",
        "extract_features(INPUT_FILE_TEST, OUTPUT_FILE_TEST, IMG_ROWS, IMG_COLS, DIR_DATASET)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihx_m47Gqt9F",
        "colab_type": "text"
      },
      "source": [
        "# IMPLEMENTANDO O SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvTgayxfq6aW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_TRAIN = \"./data/svm/train.svm\"\n",
        "INPUT_TEST = \"./data/svm/test.svm\"\n",
        "\n",
        "x_train, y_train = load_svmlight_file(INPUT_TRAIN)\n",
        "x_test, y_test = load_svmlight_file(INPUT_TEST)\n",
        "\n",
        "x_train = x_train.toarray()\n",
        "x_test = x_test.toarray()\n",
        "\n",
        "classificator = svm.SVC()\n",
        "classificator.fit(x_train, y_train)\n",
        "predict = classificator.predict(x_test)\n",
        "f1_score = round_float(sklearn_f1_score(y_test, predict, labels=np.unique(predict), average='weighted'))\n",
        "accuracy = round_float(accuracy_score(y_test, predict))\n",
        "\n",
        "print(f'Accuracy:  {accuracy}')\n",
        "print(f'F1Score:  {f1_score}')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}